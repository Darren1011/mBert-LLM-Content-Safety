{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Loading Required Libraries"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch langdetect transformers nltk openpyxl"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Requirement already satisfied: torch in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (2.5.0)\nRequirement already satisfied: langdetect in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (1.0.9)\nRequirement already satisfied: transformers in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (4.46.0)\nRequirement already satisfied: nltk in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (3.9.1)\nRequirement already satisfied: openpyxl in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (3.1.5)\nRequirement already satisfied: filelock in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch) (3.16.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch) (4.12.2)\nRequirement already satisfied: networkx in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch) (3.3)\nRequirement already satisfied: jinja2 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch) (2023.10.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch) (12.3.1.170)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch) (12.4.127)\nRequirement already satisfied: triton==3.1.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: six in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from langdetect) (1.16.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from transformers) (0.26.2)\nRequirement already satisfied: numpy>=1.17 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from transformers) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from transformers) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from transformers) (2024.9.11)\nRequirement already satisfied: requests in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from transformers) (0.20.1)\nRequirement already satisfied: tqdm>=4.27 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from transformers) (4.66.5)\nRequirement already satisfied: click in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from nltk) (8.1.7)\nRequirement already satisfied: joblib in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from nltk) (1.4.2)\nRequirement already satisfied: et-xmlfile in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from openpyxl) (2.0.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from requests->transformers) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\nNote: you may need to restart the kernel to use updated packages.\n"
        }
      ],
      "execution_count": 23,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1731989423306
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "import re\n",
        "import nltk\n",
        "from langdetect import detect"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1731981753784
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# nltk for stopword processing\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "[nltk_data] Downloading package stopwords to\n[nltk_data]     /home/azureuser/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
        }
      ],
      "execution_count": 24,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1731989425770
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Multilingual Stopwords"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize an empty set for all stopwords\n",
        "stop_words = set()\n",
        "\n",
        "# Adding English stopwords from nltk\n",
        "stop_words.update(stopwords.words('english'))\n",
        "\n",
        "# Manually define stopwords for other languages\n",
        "malay_stopwords = {\"dan\", \"adalah\", \"di\", \"ke\", \"yang\", \"ini\", \"itu\", \"saya\", \"dengan\"}  # Expand as needed\n",
        "chinese_stopwords = {\"的\", \"了\", \"在\", \"是\", \"我\", \"有\", \"和\", \"也\", \"就\"}  # Expand with Chinese stopwords\n",
        "japanese_stopwords = {\"の\", \"に\", \"は\", \"を\", \"た\", \"が\", \"で\", \"て\", \"と\"}  # Expand with Japanese stopwords\n",
        "korean_stopwords = {\"그리고\", \"그녀는\", \"나는\", \"그들\", \"이것\", \"그것\", \"우리\"}  # Expand with Korean stopwords\n",
        "thai_stopwords = {\"และ\", \"ใน\", \"เป็น\", \"ก็\", \"ว่า\", \"ได้\", \"มี\", \"หรือ\"}  # Expand with Thai stopwords\n",
        "vietnamese_stopwords = {\"và\", \"trong\", \"là\", \"của\", \"để\", \"một\", \"có\", \"không\"}  # Expand with Vietnamese stopwords\n",
        "\n",
        "# Add these lists to the main stop_words set\n",
        "stop_words.update(malay_stopwords)\n",
        "stop_words.update(chinese_stopwords)\n",
        "stop_words.update(japanese_stopwords)\n",
        "stop_words.update(korean_stopwords)\n",
        "stop_words.update(thai_stopwords)\n",
        "stop_words.update(vietnamese_stopwords)\n",
        "\n",
        "# Adding \"rt\" for retweet mentions\n",
        "stop_words.add(\"rt\")"
      ],
      "outputs": [],
      "execution_count": 25,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1731989427223
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Symbol Replacement Function"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define commonly masked symbols and their replacements\n",
        "symbol_replacements = {\"@\": \"a\", \"!\": \"i\", \"$\": \"s\", \"0\": \"o\", \"1\": \"i\"}\n",
        "\n",
        "def replace_symbols(text):\n",
        "    # Replace symbols in commonly masked words only\n",
        "    for symbol, letter in symbol_replacements.items():\n",
        "        text = re.sub(rf\"(?<!\\w){symbol}(?=\\w)|(?<=\\w){symbol}(?!\\w)\", letter, text)\n",
        "    return text"
      ],
      "outputs": [],
      "execution_count": 26,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1731989428923
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing Function with Language Detection"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_mixed_language(text):\n",
        "    # Detect language (skip if detection fails)\n",
        "    try:\n",
        "        lang = detect(text)\n",
        "    except:\n",
        "        lang = \"unknown\"\n",
        "    \n",
        "    # Replace symbols in inappropriate words while retaining URLs and special characters\n",
        "    text = replace_symbols(text)\n",
        "    \n",
        "    # Apply language-specific stopwords\n",
        "    if lang == \"en\":\n",
        "        text = \" \".join([word for word in text.split() if word.lower() not in stop_words])\n",
        "    elif lang == \"ms\":  # Malay\n",
        "        text = \" \".join([word for word in text.split() if word.lower() not in malay_stopwords])\n",
        "    elif lang == \"zh\":  # Chinese\n",
        "        text = \" \".join([word for word in text.split() if word.lower() not in chinese_stopwords])\n",
        "    elif lang == \"ja\":  # Japanese\n",
        "        text = \" \".join([word for word in text.split() if word.lower() not in japanese_stopwords])\n",
        "    elif lang == \"ko\":  # Korean\n",
        "        text = \" \".join([word for word in text.split() if word.lower() not in korean_stopwords])\n",
        "    elif lang == \"th\":  # Thai\n",
        "        text = \" \".join([word for word in text.split() if word.lower() not in thai_stopwords])\n",
        "    elif lang == \"vi\":  # Vietnamese\n",
        "        text = \" \".join([word for word in text.split() if word.lower() not in vietnamese_stopwords])\n",
        "\n",
        "    # Remove extra whitespace\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n"
      ],
      "outputs": [],
      "execution_count": 27,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1731989431943
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load and Preprocess Data"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load your dataset (adjust path as necessary)\n",
        "df = pd.read_excel('Training_LLM_Modified.xlsx')\n",
        "\n",
        "# Ensure 'Label' column only contains integers (0 or 1) by filtering out invalid rows\n",
        "df = df[df['Label'].isin([0, 1])]\n",
        "\n",
        "# Preprocess text and create 'clean_text' column\n",
        "df['clean_text'] = df['Text'].apply(preprocess_mixed_language)\n",
        "\n",
        "# Extract lists for texts and labels after cleaning\n",
        "texts = df['clean_text'].tolist()\n",
        "labels = df['Label'].astype(int).tolist()  # Convert labels to integers\n",
        "\n",
        "# Split into training, validation, and test sets\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=0.2, random_state=42)\n"
      ],
      "outputs": [],
      "execution_count": 28,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1731989446318
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenization and DataLoader Setup"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize with mBERT\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
        "val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n",
        "test_encodings = tokenizer(test_texts, truncation=True, padding=True)\n",
        "\n",
        "# Define Dataset class for PyTorch\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)  # Ensure labels are long tensors\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "# Create Dataset and DataLoader objects\n",
        "train_dataset = CustomDataset(train_encodings, train_labels)\n",
        "val_dataset = CustomDataset(val_encodings, val_labels)\n",
        "test_dataset = CustomDataset(test_encodings, test_labels)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)"
      ],
      "outputs": [],
      "execution_count": 29,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1731989450319
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Setup and Fine-Tuning"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', num_labels=2)\n",
        "model.to(device)\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
        "\n",
        "# Fine-tuning loop\n",
        "for epoch in range(5):\n",
        "    model.train()\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()\n",
        "    val_accuracy = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            predictions = torch.argmax(outputs.logits, dim=1)\n",
        "            val_accuracy += torch.sum(predictions == labels).item() / labels.size(0)\n",
        "\n",
        "    avg_val_accuracy = val_accuracy / len(val_loader)\n",
        "    print(f'Epoch {epoch + 1}, Validation Accuracy: {avg_val_accuracy}')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Epoch 1, Validation Accuracy: 0.9722222222222222\nEpoch 2, Validation Accuracy: 0.9722222222222222\nEpoch 3, Validation Accuracy: 0.9861111111111112\nEpoch 4, Validation Accuracy: 0.9742063492063492\nEpoch 5, Validation Accuracy: 0.9682539682539683\n"
        }
      ],
      "execution_count": 30,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1731992146584
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Evaluation"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "true_labels = []\n",
        "predicted_labels = []\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        predictions = torch.argmax(outputs.logits, dim=1)\n",
        "        true_labels.extend(labels.cpu().numpy())\n",
        "        predicted_labels.extend(predictions.cpu().numpy())\n",
        "\n",
        "# Classification Report and Evaluation Metrics\n",
        "print(\"Classification Report:\\n\", classification_report(true_labels, predicted_labels, target_names=['Appropriate', 'Inappropriate']))\n",
        "accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "precision = precision_score(true_labels, predicted_labels)\n",
        "recall = recall_score(true_labels, predicted_labels)\n",
        "f1 = f1_score(true_labels, predicted_labels)\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1-score: {f1}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Classification Report:\n                precision    recall  f1-score   support\n\n  Appropriate       0.95      0.99      0.97       405\nInappropriate       0.99      0.91      0.95       220\n\n     accuracy                           0.96       625\n    macro avg       0.97      0.95      0.96       625\n weighted avg       0.97      0.96      0.96       625\n\nAccuracy: 0.9648\nPrecision: 0.9852941176470589\nRecall: 0.9136363636363637\nF1-score: 0.9481132075471698\n"
        }
      ],
      "execution_count": 31,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1731992181629
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### User Prompt for Prediction "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_text(user_input):\n",
        "    # Preprocess and tokenize the input text\n",
        "    preprocessed_text = preprocess_mixed_language(user_input)\n",
        "    inputs = tokenizer(preprocessed_text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=128)\n",
        "    \n",
        "    # Move inputs to device (CPU or GPU as available)\n",
        "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
        "    \n",
        "    # Predict with the model\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        probabilities = torch.softmax(outputs.logits, dim=1)\n",
        "        predicted_label = torch.argmax(probabilities, dim=1).item()\n",
        "        confidence = probabilities[0][predicted_label].item()\n",
        "\n",
        "    # Output results\n",
        "    label_text = \"Inappropriate\" if predicted_label == 1 else \"Appropriate\"\n",
        "    print(f\"Text Classification: {label_text} (Label: {confidence:.2f})\")\n",
        "\n",
        "# Example usage\n",
        "user_input = input(\"Enter text to classify: \")\n",
        "classify_text(user_input)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Text Classification: Appropriate (Label: 0.76)\n"
        }
      ],
      "execution_count": 49,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1732000245650
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save Model for ONNX Conversion "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_save_path = \"fine_tuned_mBERT_model\"\n",
        "model.save_pretrained(model_save_path)\n",
        "tokenizer.save_pretrained(model_save_path)\n",
        "print(f\"Model saved to {model_save_path} for ONNX conversion.\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Model saved to fine_tuned_mBERT_model for ONNX conversion.\n"
        }
      ],
      "execution_count": 33,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1731995869117
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "print(f\"Python version: {sys.version}\")\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1731931333109
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}